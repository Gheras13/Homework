{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gheras13/Homework/blob/main/Homewrok_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MACHINE LEARNING HOMEWORK 1\n",
        "\n",
        "#DIRECT KINEMATICS OF ROBOT MANIPLUATOR"
      ],
      "metadata": {
        "id": "7u7FEl8kWjQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GMC0IvrZxBSH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3BjXK3Q2vtj",
        "outputId": "3863329a-c181-49e5-a557-2251cf08ba9e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Data"
      ],
      "metadata": {
        "id": "WTdw6AwT2Ydx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Percorso del file CSV\n",
        "file_path = '/content/drive/My Drive/File_csv/Copia di r2_20_100k.csv'\n",
        "# Carica il dataset\n",
        "dataset = pd.read_csv(file_path, sep=';', header=0)\n",
        "\n",
        "# Rimuovi eventuali spazi dai nomi delle colonne\n",
        "dataset.columns = dataset.columns.str.strip()\n",
        "#prendo i primi 1000 samples\n",
        "df = dataset.iloc[:30000]\n",
        "# Stampa i nomi delle colonne disponibili\n",
        "print(\"Colonne disponibili nel dataset:\", df.columns.tolist())\n",
        "\n",
        "# Estrai le feature (j0, j1) e i target (ee_x, ee_y)\n",
        "X = df[['j0', 'j1']].values  # Angoli articolari\n",
        "Y = df[['ee_x', 'ee_y']].values  # Posizioni finali (end-effector)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqA2qADU2Tfg",
        "outputId": "973ce7dc-9115-4314-a6c6-7df411941c77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colonne disponibili nel dataset: ['j0', 'j1', 'cos(j0)', 'cos(j1)', 'sin(j0)', 'sin(j1)', 'ee_x', 'ee_y', 'ee_qw', 'ee_qz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividi i dati in training e testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "Ez9Y2iKpMJqJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test del modello"
      ],
      "metadata": {
        "id": "HfIHF2z-WH8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rete neurale FeedForward con tensorflow\n",
        "\n",
        "Definisce una rete neurale feedforward in cui i layer sono collegati uno dopo l'altro (architettura sequenziale).\n",
        "Ogni layer prende in input l'output del layer precedente.\n",
        "\n",
        "Definizione di una rete neurale feedforward per la cinematica diretta.\n",
        " Struttura:\n",
        " - Input: 2 caratteristiche (angoli articolari).\n",
        " - 3 layer nascosti con attivazione ReLU.\n",
        " - Output: 2 caratteristiche (posizione terminale x, y).\n"
      ],
      "metadata": {
        "id": "CLwmCkjQWZ_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BASE_MODEL, DROPOUT, WEIGHT_DECAY"
      ],
      "metadata": {
        "id": "873vOn9dYsLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hidden_neurons, learning_rate, dropout_rate=None, l2_lambda=None):\n",
        "    \"\"\"\n",
        "    Crea un modello configurabile con dropout e regolarizzazione L2.\n",
        "\n",
        "    Args:\n",
        "        hidden_neurons (int): Numero di neuroni negli strati nascosti.\n",
        "        learning_rate (float): Tasso di apprendimento.\n",
        "        dropout_rate (float, optional): Percentuale di dropout (None per nessun dropout).\n",
        "        l2_lambda (float, optional): Coefficiente di regolarizzazione L2 (None per nessuna regolarizzazione).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Sequential: Modello compilato.\n",
        "    \"\"\"\n",
        "    layers = [\n",
        "        tf.keras.layers.Dense(\n",
        "            hidden_neurons, activation='relu', input_shape=(2,),\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2_lambda) if l2_lambda else None\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    if dropout_rate:\n",
        "        layers.append(tf.keras.layers.Dropout(dropout_rate))  # Aggiungi Dropout\n",
        "\n",
        "    layers.extend([\n",
        "        tf.keras.layers.Dense(\n",
        "            hidden_neurons, activation='relu',\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l2_lambda) if l2_lambda else None\n",
        "        ),\n",
        "        tf.keras.layers.Dense(2)  # Output: posizione (x, y)\n",
        "    ])\n",
        "\n",
        "    model = tf.keras.Sequential(layers)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "HBwcCHbeit46"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Configurazione del modello per l'addestramento:\n",
        " - Ottimizzatore: Adam (learning_rate=0.01).\n",
        " - Funzione di perdita: Mean Squared Error (MSE).\n",
        " - Metriche: Mean Absolute Error (MAE).\n"
      ],
      "metadata": {
        "id": "HAczh-_rX-yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valutazione finale del modello sul set di test.\n",
        "# Metriche:\n",
        "# - `loss`: Errore quadratico medio (MSE) perchè siamo nella regressione, risulterà essere la nostra LOSS FUNCTION.\n",
        "# - `mae`: Errore assoluto medio.\n",
        "# Come SOLVER è stato scelto ADAM,SGD\n"
      ],
      "metadata": {
        "id": "__wD-_mYYI0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GRID SEARCH"
      ],
      "metadata": {
        "id": "BCnHnwRqY8Td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "\n",
        "# Parametri per il tuning del modello base\n",
        "hidden_layers = [64, 128]\n",
        "learning_rates = [0.01, 0.001]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Tuning sul modello base...\")\n",
        "for hl, lr in product(hidden_layers, learning_rates):\n",
        "    print(f\"Testing Baseline Model with {hl} hidden neurons and {lr} learning rate\")\n",
        "\n",
        "    # Costruisci il modello base\n",
        "    model_baseline = build_model(hidden_neurons=hl, learning_rate=lr)\n",
        "    history = model_baseline.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=32, verbose=0)\n",
        "    loss, mae = model_baseline.evaluate(X_test, Y_test, verbose=0)\n",
        "    results.append((hl, lr, loss, mae))\n",
        "\n",
        "# Riporta i risultati migliori per il modello base\n",
        "sorted_results = sorted(results, key=lambda x: x[2])  # Ordina per perdita\n",
        "best_config = sorted_results[0]\n",
        "print(f\"Miglior configurazione trovata per il modello base: Neuroni Nascosti={best_config[0]}, Learning Rate={best_config[1]}\")\n"
      ],
      "metadata": {
        "id": "qkAIjYvvNK98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d6c2db-e960-4a2e-b3c6-bd865b7ca5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning sul modello base...\n",
            "Testing Baseline Model with 64 hidden neurons and 0.01 learning rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COMPILE THE MODEL"
      ],
      "metadata": {
        "id": "t98micLpZiad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametri ottimali trovati dal tuning\n",
        "optimal_hidden_neurons = best_config[0]\n",
        "optimal_learning_rate = best_config[1]\n",
        "\n",
        "# Configurazioni per Dropout e Weight Decay\n",
        "dropout_rate = 0.2\n",
        "l2_lambda = 0.001\n",
        "\n",
        "# Lista per i risultati finali\n",
        "final_results = []\n",
        "\n",
        "# 1. Modello Base\n",
        "print(\"\\nAddestramento Modello Base...\")\n",
        "model_baseline = build_model(hidden_neurons=optimal_hidden_neurons, learning_rate=optimal_learning_rate)\n",
        "history_baseline = model_baseline.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=32, verbose=0)\n",
        "loss, mae = model_baseline.evaluate(X_test, Y_test, verbose=0)\n",
        "final_results.append(('Baseline', loss, mae))\n",
        "\n",
        "# 2. Modello con Dropout\n",
        "print(\"\\nAddestramento Modello con Dropout...\")\n",
        "model_dropout = build_model(hidden_neurons=optimal_hidden_neurons, learning_rate=optimal_learning_rate, dropout_rate=dropout_rate)\n",
        "history_dropout = model_dropout.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=32, verbose=0)\n",
        "loss, mae = model_dropout.evaluate(X_test, Y_test, verbose=0)\n",
        "final_results.append(('Dropout', loss, mae))\n",
        "\n",
        "# 3. Modello con Weight Decay\n",
        "print(\"\\nAddestramento Modello con Weight Decay...\")\n",
        "model_l2 = build_model(hidden_neurons=optimal_hidden_neurons, learning_rate=optimal_learning_rate, l2_lambda=l2_lambda)\n",
        "history_l2 = model_l2.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=32, verbose=0)\n",
        "loss, mae = model_l2.evaluate(X_test, Y_test, verbose=0)\n",
        "final_results.append(('Weight Decay', loss, mae))\n"
      ],
      "metadata": {
        "id": "Ae5-iXNMZD-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizza i risultati finali\n",
        "print(\"\\nConfronto Finale tra i Modelli:\")\n",
        "for res in final_results:\n",
        "    print(f\"Modello: {res[0]}, Loss: {res[1]:.4f}, MAE: {res[2]:.4f}\")\n",
        "\n",
        "# Grafico per il Modello Base\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history_baseline.history['loss'], label='Training Loss')\n",
        "plt.plot(history_baseline.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Modello Baseline - Curve di Perdita')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Grafico per il Modello Dropout\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history_dropout.history['loss'], label='Training Loss', linestyle='dashed')\n",
        "plt.plot(history_dropout.history['val_loss'], label='Validation Loss', linestyle='dashed')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Modello Dropout - Curve di Perdita')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Grafico per il Modello Weight Decay\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history_l2.history['loss'], label='Training Loss', linestyle='dotted')\n",
        "plt.plot(history_l2.history['val_loss'], label='Validation Loss', linestyle='dotted')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Modello Weight Decay - Curve di Perdita')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EIEyNXZPZn9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definizione dei jacobiani"
      ],
      "metadata": {
        "id": "ZcAFEV4udTou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def FK_Jacobian(model, x):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch(x)\n",
        "        y = FK(model, x)  # Forward pass\n",
        "    return tape.jacobian(y, x)\n",
        "\n",
        "def FK(model, theta):\n",
        "    # Reshape to batch size 1\n",
        "    t = tf.reshape(theta, shape=(1, 2))\n",
        "    out = model(t)\n",
        "    # Reshape to 1d vector\n",
        "    out = tf.reshape(out, shape=(2,))\n",
        "    return out\n",
        "\n",
        "def analytical_J(theta, L1=0.1, L2=0.1):\n",
        "    j0, j1 = theta\n",
        "    J = np.array([\n",
        "        [-L1 * np.sin(j0) - L2 * np.sin(j0 + j1), -L2 * np.sin(j0 + j1)],\n",
        "        [L1 * np.cos(j0) + L2 * np.cos(j0 + j1),  L2 * np.cos(j0 + j1)]\n",
        "    ])\n",
        "    return J\n"
      ],
      "metadata": {
        "id": "nrAnGxbfNDaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definizione dei modelli già addestrati\n",
        "models = {\n",
        "    \"Baseline\": model_baseline,\n",
        "    \"Dropout\": model_dropout,\n",
        "    \"Weight Decay\": model_l2\n",
        "}\n",
        "\n",
        "# Parametri del robot\n",
        "L1, L2, L3 = 0.1, 0.1, 0.1  # Lunghezze dei link (modifica se necessario)\n",
        "\n",
        "# Campione di test\n",
        "theta_sample = X_test[0]  # Primo esempio di test\n",
        "\n",
        "# Confronto per ciascun modello\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nConfronto per il modello: {model_name}\")\n",
        "\n",
        "    # Calcolo della Jacobiana appresa\n",
        "    jacobian_learned = FK_Jacobian(model, tf.convert_to_tensor([theta_sample], dtype=tf.float32))\n",
        "\n",
        "    # Calcolo della Jacobiana analitica\n",
        "    jacobian_analytical = analytical_J(theta_sample, L1, L2)\n",
        "\n",
        "    # Flatten delle matrici\n",
        "    jacobian_learned_flat = jacobian_learned.numpy().flatten()\n",
        "    jacobian_analytical_flat = jacobian_analytical.flatten()\n",
        "\n",
        "    # Calcolo del MSE\n",
        "    mse = np.mean((jacobian_learned_flat - jacobian_analytical_flat) ** 2)\n",
        "\n",
        "\n",
        "    # Visualizza le Jacobiane\n",
        "    print(\"\\nJacobiana appresa:\\n\", jacobian_learned.numpy())\n",
        "    print(\"\\nJacobiana analitica:\\n\", jacobian_analytical)\n",
        "    print(\"\\nMean Squared Error (MSE): \",mse)"
      ],
      "metadata": {
        "id": "-5foge8iNI3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}